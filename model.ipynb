{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 260,
   "outputs": [],
   "source": [
    "###############################################################\n",
    "# Author: walterwhites\n",
    "# Hackathon: CrackedDevs Hackathon Jan 2024\n",
    "# This code is subject Devpost Hackathon and restrictions.\n",
    "###############################################################"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-19T23:25:08.803910Z",
     "start_time": "2024-01-19T23:25:08.795703Z"
    }
   },
   "id": "c804ee1bcfda6ab7"
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('jobs_data.csv')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-19T23:25:08.836060Z",
     "start_time": "2024-01-19T23:25:08.800806Z"
    }
   },
   "id": "initial_id"
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import sent_tokenize, WordNetLemmatizer\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "def search(query, model, descriptions):\n",
    "    query = preprocess_text(query)\n",
    "    query_vector = np.mean([model.wv[word] for word in query.split() if word in model.wv], axis=0)\n",
    "\n",
    "    description_vectors = [np.mean([model.wv[word] for word in desc.split() if word in model.wv], axis=0) for desc in descriptions]\n",
    "\n",
    "    cosine_similarities = [linear_kernel([query_vector], [desc_vector]).flatten()[0] for desc_vector in description_vectors]\n",
    "\n",
    "    results = pd.DataFrame({'id': df['id'], 'Description': df['cleaned_description'], 'Similarity': cosine_similarities})\n",
    "    results = results.sort_values(by='Similarity', ascending=False)\n",
    "    \n",
    "    return results\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = lowercase_text(text)\n",
    "    text = remove_urls(text)\n",
    "    # Tokenization\n",
    "    word_tokens = tokenization(text)\n",
    "    word_tokens = clear_ponctuation(word_tokens)\n",
    "    word_tokens = custom_clean(word_tokens)\n",
    "    # Lemmatisation\n",
    "    lemmatized_words = lemmatization(word_tokens)\n",
    "    # Suppression des stopwords\n",
    "    cleaned_text = remove_stopwords(lemmatized_words)\n",
    "    cleaned_text_str = ' '.join(cleaned_text)  # Convert the list to a string\n",
    "    return cleaned_text_str\n",
    "\n",
    "def remove_urls(text):\n",
    "    regex = r'https?://\\S+|www\\.\\S+'\n",
    "    text = re.sub(regex, '', text)\n",
    "    return text\n",
    "\n",
    "def lowercase_text(text):\n",
    "    return text.lower()\n",
    "\n",
    "def lemmatization(word_tokens):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return [lemmatizer.lemmatize(word) for word in word_tokens]\n",
    "\n",
    "def tokenization(text):\n",
    "    return sent_tokenize(text)\n",
    "\n",
    "def custom_clean(text):\n",
    "    # Remplacer les sauts de lignes par des espaces\n",
    "    word_tokens = [word_token.replace('\\n', ' ') for word_token in text]\n",
    "    # Remplacer les non-breaking space (nbsp) par des espaces\n",
    "    word_tokens = [re.sub(r'\\xa0', ' ', phrase) for phrase in word_tokens]\n",
    "    # Supprimer les espaces en trop\n",
    "    word_tokens = [re.sub(r'\\s+', ' ', phrase) for phrase in word_tokens]\n",
    "    return word_tokens\n",
    "\n",
    "def remove_stopwords(sentences):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    result = []\n",
    "    for sentence in sentences:\n",
    "        words = sentence.split()\n",
    "        filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "        result.append(' '.join(filtered_words))\n",
    "    return result\n",
    "\n",
    "def clear_ponctuation(text):\n",
    "    return [re.sub(r'[^a-zA-Z0-9\\s]', '', phrase) for phrase in text]\n",
    "\n",
    "def extract_text_from_body(html_body):\n",
    "    soup = BeautifulSoup(html_body, 'html.parser')\n",
    "    return soup.get_text()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-19T23:25:08.844255Z",
     "start_time": "2024-01-19T23:25:08.842041Z"
    }
   },
   "id": "9407ca7fc6bb8038"
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "outputs": [],
   "source": [
    "# Clean HTML tags\n",
    "df['description'] = df['description'].apply(extract_text_from_body)\n",
    "\n",
    "# Clean Text \n",
    "df['cleaned_description'] = df['description'].apply(preprocess_text)\n",
    "df[['id', 'cleaned_description']].to_csv('cleaned_jobs_data.csv', index=False)\n",
    "df.to_pickle(\"api/cleaned_jobs_data.pkl\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-19T23:25:08.937100Z",
     "start_time": "2024-01-19T23:25:08.855666Z"
    }
   },
   "id": "57fe43cf65ed7385"
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      id                                        Description  Similarity\n",
      "29  3082  we3 leading professional network women nonbina...   31.929531\n",
      "27  3084  building talent pool highly skilled backend en...   31.259947\n",
      "26  3095  rampcom est compensation 150kyr series 300 emp...   30.442137\n",
      "31  3077  building talent pool highly skilled frontend e...   30.432049\n",
      "30  3080  series c combinator wellness company est compe...   30.056515\n",
      "4   3970  seeking highly motivated individual join team ...   28.836357\n",
      "25  3098  rampcom est compensation 150kyr series 300 emp...   28.411894\n",
      "24  3100  job description remote position moonward capit...   27.862480\n",
      "37  3006  coingecko global leader tracking cryptocurrenc...   25.193859\n",
      "0   4042  search passionate organized individual take ro...   24.488400\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "model = Word2Vec(sentences=df['cleaned_description'].apply(lambda x: x.split()), vector_size=200, window=10, min_count=1, workers=4, epochs=20)\n",
    "\n",
    "query = \"blockchain\"\n",
    "search_results = search(query, model, df['cleaned_description'])\n",
    "print(search_results.head(10))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-19T23:25:09.519051Z",
     "start_time": "2024-01-19T23:25:08.958123Z"
    }
   },
   "id": "159126c79d0d0310"
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "outputs": [
    {
     "data": {
      "text/plain": "['model_Word2Vec.joblib']"
     },
     "execution_count": 265,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump\n",
    "\n",
    "dump(model, 'model_Word2Vec.joblib')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-01-19T23:25:09.555652Z",
     "start_time": "2024-01-19T23:25:09.517865Z"
    }
   },
   "id": "55b2743c8ba26ea8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
